{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information Retrieval   |    Assignment 1-B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Wolf, Florian - 12393339 (UvA) flocwolf@gmail.com\n",
    "  \n",
    "  Groot Kormelink, Joseph - 11784903 (UvA) josephgk@hotmail.nl\n",
    " \n",
    "  Rostov, Maxim - 11808470 (UvA) maxim.rostan@gmail.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import random as rnd\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from functools import reduce\n",
    "import math\n",
    "from scipy import stats\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Open file    \n",
    "fileHandler = open ('YandexRelPredChallenge.txt', \"r\")\n",
    "# Get list of all lines in file\n",
    "listOfLines = fileHandler.readlines()\n",
    "# Close file \n",
    "fileHandler.close()\n",
    "#Empty frame to fill in\n",
    "index=[x for x in range(0,100000)]\n",
    "df = pd.DataFrame(index=index, columns=['SessionID', 'TimePassed',\n",
    "                 'TypeOfAction', 'QueryID','RegionID', 'ListOfURLs', 'URLID'])\n",
    "df.name='data'\n",
    "\n",
    "# Iterate over the lines and fill in the dataframe\n",
    "for ind, line in zip(index, listOfLines):\n",
    "#     print('index')\n",
    "    line = line.strip().split()\n",
    "    if line[2]=='Q':\n",
    "\n",
    "        df.loc[ind] = pd.Series({'SessionID':line[0], 'TimePassed':line[1], 'TypeOfAction':line[2],\n",
    "                                     'QueryID':line[3],'RegionID':line[4], 'ListOfURLs':line[5:], 'URLID':np.nan})\n",
    "    else:\n",
    "        df.loc[ind] = pd.Series({'SessionID':line[0], 'TimePassed':line[1], 'TypeOfAction':line[2],\n",
    "                                     'QueryID':np.nan,'RegionID':np.nan, 'ListOfURLs':np.nan, 'URLID':line[3:]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Getting the types right. \n",
    "df.SessionID = df.SessionID.astype(int)\n",
    "df.TimePassed = df.TimePassed.astype(int)\n",
    "df.TypeOfAction = df.TypeOfAction.astype(str)\n",
    "df.RegionID = df.RegionID.astype(float)\n",
    "df.TimePassed = df.TimePassed.astype(float)\n",
    "def parse_ulrid(x):\n",
    "    if type(x)==list:\n",
    "        return x[0]\n",
    "    else:\n",
    "        return x\n",
    "df.URLID = df.URLID.apply(parse_ulrid)\n",
    "df.URLID = df.URLID.astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create new relevance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create new relevance scores \n",
    "relevances = [0,1]\n",
    "doc_length = 3 \n",
    "MODELS =[\"E\",\"P\"]\n",
    "relevance_pairs=[\"\".join(seq) for seq in itertools.product(\"01\", repeat=6)]\n",
    "relevance_pairs = [(p[:3],p[3:])for p in relevance_pairs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Calculate ERR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ERR(l):\n",
    "    theta = [ (2**i - 1)/(2) for i in [0,1]]\n",
    "    err=0\n",
    "    \n",
    "    for rank in range(len(l)):\n",
    "        prod = 1\n",
    "        for idx in range(rank):\n",
    "            prod*=(1-theta[int(l[idx])])\n",
    "        prod*=(1/(rank+1))*theta[int(l[rank])]\n",
    "        err+=prod\n",
    "    return err\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "see the formuala for ERR at: http://olivier.chapelle.cc/pub/err.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the deltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# All below 0.05 is discarded The numbers are the upperbound\n",
    "binned = { x/100:[]for x in range(10,100,5) }\n",
    "for tup in relevance_pairs:\n",
    "    d_err = ERR(tup[0]) - ERR(tup[1])\n",
    "    if d_err<0.05:\n",
    "        continue # discard this entry\n",
    "    \n",
    "    d_err =int(5 * round((d_err*100)/5))/100\n",
    "    binned[d_err].append(tup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_possible_permutations(tup):\n",
    "    ids=['a','b','c','d','e','f']\n",
    "    pair1 = ['a','b','c']\n",
    "    rel1  = tup[0]\n",
    "    rel2  = tup[1]\n",
    "    possible_pairs = []\n",
    "    \n",
    "    possible_ids = []\n",
    "    for idx in range(3):\n",
    "        possible_ids.append([(ids[idx+3],rel2[idx])])\n",
    "        for _id,r1 in enumerate(rel1):\n",
    "                if r1==rel2[idx]:\n",
    "                    possible_ids[idx].append((ids[_id],rel2[idx]))#find all possible overlapping results\n",
    "    \n",
    "    for l0 in possible_ids[0]:\n",
    "        for l1 in possible_ids[1]:\n",
    "            for l2 in possible_ids[2]:\n",
    "                if l0 is not l1 is not l2 is not l0:\n",
    "                    possible_pairs.append([l0,l1,l2])\n",
    "    pairs = []\n",
    "    for p in possible_pairs:\n",
    "        pairs.append(([ tup for tup in zip(pair1,rel1)],p))\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def team_draft(tup,start=0,):\n",
    "    l = []\n",
    "    tup = (list(tup[0]),list(tup[1]))\n",
    "    for idx in range(6):\n",
    "        start+=1\n",
    "        if len(tup[start%2])==0:continue\n",
    "        elem = tup[start%2].pop(0)\n",
    "        tup = (list(filter(lambda a: a != elem, tup[0])), list(filter(lambda a: a != elem, tup[1])))\n",
    "        l.append((elem +(MODELS[start%2],)))\n",
    "    return l\n",
    "\n",
    "def probabelistic_interleaving(tup):\n",
    "    l = []\n",
    "    tup = (list(tup[0]),list(tup[1]))\n",
    "\n",
    "    for x in range(6):\n",
    "        active=[]\n",
    "        if(len(tup[0]))>0:active+=[0]\n",
    "        if(len(tup[1]))>0:active+=[1]    \n",
    "        if not active:break\n",
    "        cur = np.random.choice(active) # get the currently selected model\n",
    "        \n",
    "        unnormalized_probs=[]\n",
    "        for idx in range(len(tup[cur])):\n",
    "            unnormalized_probs.append(1/((idx+1)**3))#tau is 3 from the paper\n",
    "        unnormalized_probs = [x/sum(unnormalized_probs) for x in unnormalized_probs]\n",
    "        elemz_idx = np.random.choice(range(len(tup[cur])),p=unnormalized_probs)\n",
    "        elem = tup[cur][idx]\n",
    "        \n",
    "        tup = (list(filter(lambda a: a != elem, tup[0])), list(filter(lambda a: a != elem, tup[1])))\n",
    "        l.append((elem + (MODELS[cur],)))\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('b', '1', 'P'), ('d', '1', 'P'), ('c', '1', 'E'), ('a', '1', 'E')]\n",
      "[('d', '1', 'P'), ('a', '1', 'E'), ('b', '1', 'P'), ('c', '1', 'E')]\n"
     ]
    }
   ],
   "source": [
    "l = create_possible_permutations(['111','111'])\n",
    "print(probabelistic_interleaving(l[10]))\n",
    "print(team_draft(l[10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4.1: Estimating the params of click models: Random Click Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "proability for a doc to be clicked:  0.13445559411047547\n"
     ]
    }
   ],
   "source": [
    "# Get all the doc ids\n",
    "data = df.copy() \n",
    "data.QueryID = data.QueryID.astype(float)\n",
    "list_doc_ids_raw = data.ListOfURLs[df.ListOfURLs.isnull()!=True].values\n",
    "res = list()\n",
    "for i in list_doc_ids_raw:\n",
    "    res += i\n",
    "all_doc_ids = res.copy()\n",
    "# Get all the clicks\n",
    "list_clicks_ids = data.URLID[df.URLID.isnull()!=True].values\n",
    "\n",
    "# rho = probability of a doc to be clicked.\n",
    "rho = len(list_clicks_ids)/len(all_doc_ids)\n",
    "print('proability for a doc to be clicked: ', rho)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4.2: Estimating the params of click models: Posistion-based Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the data for training\n",
    "    - assumption: we assume that all the queries have at least n_docs in them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = df.copy()\n",
    "data.QueryID = data.QueryID.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the tables with all the params\n",
    "n_docs = 6\n",
    "unique_queries = data.QueryID[data.QueryID.isnull()!=True].astype(int).unique()\n",
    "n_queries = len(unique_queries) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", but still kinda quick tho : seconds spent 79.07310748100281, mins spent 1.31788512468338\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# Create query_doc empty pairs\n",
    "query_doc_pairs = defaultdict()\n",
    "for i in unique_queries:\n",
    "    query_doc_pairs[i]=[]\n",
    "    \n",
    "# This code will give an error at the very last session ! -> move last session to the different bit.\n",
    "sessions = data.SessionID.unique()[:-1]\n",
    "for session in sessions:\n",
    "    data_session = data[data.SessionID==session]\n",
    "    click_ids =  data_session.URLID[data_session.URLID.isnull()!=True].values\n",
    "    query_ids = data_session.QueryID[data_session.QueryID.isnull()!=True].values\n",
    "    for query_id in query_ids:\n",
    "        first_n_docs = data_session.ListOfURLs[data.QueryID==query_id].iloc[0]\n",
    "        first_n_docs = np.array(list(map(int,first_n_docs))[:n_docs])\n",
    "        # take index in the current data_session\n",
    "        query_id_ind = data_session.QueryID[data_session.QueryID==query_id].index[0]\n",
    "        flag = not math.isnan(data.URLID.iloc[query_id_ind+1])\n",
    "        # if the next entry is click\n",
    "        if flag:\n",
    "            click_log = np.zeros(n_docs)\n",
    "            query_id_ind+=1\n",
    "            while flag==1:\n",
    "                click_urlid = data.URLID.iloc[query_id_ind]\n",
    "                # if click_urlid in first_n_docs then click is 1 on that position.\n",
    "                doc_u_id = np.where(first_n_docs==click_urlid)\n",
    "                click_log[doc_u_id]=1\n",
    "                # increase the count\n",
    "                query_id_ind +=1\n",
    "                flag = not math.isnan(data.URLID.iloc[query_id_ind])\n",
    "        else:\n",
    "            click_log = np.zeros(n_docs)\n",
    "            \n",
    "        # Save query_id-click_log (=session-doc) pair for each query_id\n",
    "        query_doc_pairs[query_id].append(click_log)\n",
    "    \n",
    "end = time.time()\n",
    "print(f'We only need to do this ones: seconds spent {end - start}, mins spent {(end - start)/60}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check that for each query-doc we have a click-log:\n",
    "# query_doc_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform EM steps (update assignments ; update params):\n",
    "    \n",
    "    1) initialize alpha and gamma at random \n",
    "    2) For each time step:\n",
    "        - Expecation: do not really have to do this step. What we want to see at the end: P(C|u) \n",
    "          Calculate P(C|u) for each u (u is from 1 to n = positions in the shown list)\n",
    "          \"P(C|u) = P(A_uq|E,u)P(E|u)\"\n",
    "                \n",
    "        - Maximization: update alpha and gamma params according to the formalas \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize params\n",
    "alphas = np.random.rand(n_docs*n_queries).reshape((n_docs, n_queries))\n",
    "gammas = np.random.rand(n_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alphas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert unique_queries.shape[0] == alphas.shape[1], 'Error'\n",
    "\n",
    "# for comparison :\n",
    "alphas_old = alphas.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seconds spent 0.7984335422515869, mins spent 0.013307225704193116\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# Do the updates for alphas:\n",
    "\n",
    "def train_alphas(alphas, gammas, query_doc_pairs, n_docs):\n",
    "    # refer to alphas array with query_num as that it is how the are strored there:\n",
    "    for query_num, query_id in enumerate(query_doc_pairs):\n",
    "        for doc_u in range(n_docs):\n",
    "            factor_1 = 1/len(query_doc_pairs[query_id])\n",
    "            summ =  0\n",
    "            for sessions_docs in query_doc_pairs[query_id]:\n",
    "                click = sessions_docs[doc_u]\n",
    "                summ += click + (1-click)*( (1-gammas[doc_u])*alphas[doc_u][query_num] ) / \\\n",
    "                                            (1-gammas[doc_u]*alphas[doc_u][query_num])\n",
    "                \n",
    "            alphas[doc_u][query_num] = factor_1*summ\n",
    "            \n",
    "    return alphas\n",
    "\n",
    "alphas = train_alphas(alphas, gammas, query_doc_pairs, n_docs)\n",
    "\n",
    "end = time.time()\n",
    "print(f'seconds spent {end - start}, mins spent {(end - start)/60}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is that true that we ve changed every alpha ? -> True\n"
     ]
    }
   ],
   "source": [
    "# Is that true that we ve changed every alpha ?\n",
    "print('Is that true that we ve changed every alpha ? ->', not (alphas_old == alphas).any())\n",
    "assert alphas_old.shape == alphas.shape, 'new shapes are incorrect'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gammas\n",
    "\n",
    "We normalize by all the sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_sess = [ query_doc_pairs[idd] for idd in query_doc_pairs]\n",
    "all_sess = reduce(lambda x,y: x+y, all_sess)\n",
    "\n",
    "# for comparison :\n",
    "gammas_old = gammas.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seconds spent 0.8707931041717529, mins spent 0.014513218402862548\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# Do the updates for gammas:\n",
    "\n",
    "def train_gammas(alphas, gammas, query_doc_pairs, n_docs):\n",
    "    for doc_u in range(n_docs):\n",
    "        # refer to alphas array with query_num as that it is how the are strored there:\n",
    "        summ =  0\n",
    "        for query_num, query_id in enumerate(query_doc_pairs):\n",
    "            factor_1 = 1 / len(all_sess)\n",
    "            for sessions_docs in query_doc_pairs[query_id]:\n",
    "                click = sessions_docs[doc_u]\n",
    "                summ += click + (1-click)*( (1-alphas[doc_u][query_num])*gammas[doc_u] ) / \\\n",
    "                                            (1-gammas[doc_u]*alphas[doc_u][query_num])        \n",
    "        gammas[doc_u] = factor_1 * summ\n",
    "    \n",
    "    return gammas\n",
    "\n",
    "gammas = train_gammas(alphas, gammas, query_doc_pairs, n_docs)\n",
    "\n",
    "end = time.time()\n",
    "print(f'seconds spent {end - start}, mins spent {(end - start)/60}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "should be all true, coz otherwise we have queries with no sessions [ True]\n"
     ]
    }
   ],
   "source": [
    "# Test to see if we have all queries filled in\n",
    "def check_sanity(l):\n",
    "    if l==[]:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "test = pd.Series(query_doc_pairs)\n",
    "test_res = test.apply(check_sanity)\n",
    "print('should be all true, coz otherwise we have queries with no sessions', np.unique(np.array(test_res)))\n",
    "\n",
    "assert gammas_old.shape == gammas.shape, 'shapes are incorrect'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.69307551,  0.77839685,  0.54004278,  0.26159686,  0.99377931,\n",
       "        0.50940277])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gammas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training multiple times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now real training: \n",
    "gammas = np.random.rand(n_docs)\n",
    "alphas = np.random.rand(n_docs*n_queries).reshape((n_docs,n_queries))\n",
    "\n",
    "#  Training till convergence: \n",
    "n_epochs = 5\n",
    "diff_alphas = [1]\n",
    "diff_gammas = [1]\n",
    "\n",
    "for i in range(n_epochs):\n",
    "    gammas = train_gammas(alphas, gammas, query_doc_pairs, n_docs)\n",
    "    alphas = train_alphas(alphas, gammas, query_doc_pairs, n_docs)\n",
    "    \n",
    "    #For plotting: checking convergence\n",
    "    diff_alphas.append( diff_alphas[-1] - sum(sum(alphas)) )\n",
    "    diff_gammas.append( diff_gammas[-1] - sum(gammas) )\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.83336596,  0.41132522,  0.30714223,  0.63079465,  0.29293541,\n",
       "        0.55439753])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gammas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5\n",
    "\n",
    "Power analysis:\n",
    "interleave and simulate clicks on every permutation for $k$ times. Then calculate winning prbability $p$. Do the Power Analysis to find minimum number of sample size $N$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Variables\n",
    "\n",
    "\n",
    "k = 50\n",
    "rnd.seed(21)\n",
    "#gammas = [0.484221138516365, 0.21930976273093877, 0.1581402982275157]\n",
    "gammas_copy = gammas\n",
    "#gammas = gammas_copy\n",
    "epsilon = 0.05\n",
    "\n",
    "p0 = 0.5\n",
    "alphaN = 0.05\n",
    "betaN = 0.1\n",
    "za = 0.8289\n",
    "zb = 0.8159\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# if the probability of the PBM is higher than a ranomly generates number we assume a click happens\n",
    "\n",
    "def sim_clicks(rankList):\n",
    "    pList = np.ones(len(rankList))\n",
    "    for n in range(len(pList)):\n",
    "        if rankList[n] == 1:\n",
    "            pList[n]*= (1-epsilon)*gammas[n]\n",
    "        else:\n",
    "            pList[n]*= epsilon*gammas[n]\n",
    "    randP = np.random.uniform(size=len(pList))\n",
    "    click = np.zeros(len(pList))\n",
    "    for n in range(len(pList)):\n",
    "        if pList[n] - randP[n] > 0:\n",
    "            click[n] = 1\n",
    "    return click\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_click_model(rankList):\n",
    "    clicks = np.ones(len(rankList))\n",
    "    for idx in range(len(rankList)):\n",
    "        clicks[idx] =  int(random.getrandbits(1))\n",
    "    return clicks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_size(p1):\n",
    "\n",
    "    delta = np.absolute(p1-p0)\n",
    "    N1 = np.power(((za - alphaN*np.sqrt(p0*(1.0-p0))+zb-betaN*np.sqrt(p1*(1.0-p1)))/delta),2)\n",
    "    N = np.ceil(N1 + (1/delta))\n",
    "    return N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "def poweranal(tup,clickmodel=sim_clicks):\n",
    "    winsTDE = 0\n",
    "    winsPE = 0\n",
    "    winsTDP = 0\n",
    "    winsPP = 0\n",
    "    for n in range(k):\n",
    "        rankTD = team_draft(tup,int(random.getrandbits(1)))\n",
    "        rankP = probabelistic_interleaving(tup)\n",
    "\n",
    "        relTD = np.zeros(len(rankTD))\n",
    "        relP = np.zeros(len(rankP))\n",
    "        for m in range(len(rankTD)):\n",
    "            line = rankTD[m]\n",
    "            relTD[m] = line[1]\n",
    "        for m in range(len(rankP)):\n",
    "            line = rankP[m]\n",
    "            relP[m] = line[1]\n",
    "\n",
    "        clicksTDE = 0\n",
    "        clicksTDP = 0\n",
    "        clicksPE = 0\n",
    "        clicksPP = 0\n",
    "        while clicksTDE==clicksTDP or clicksPE==clicksPP:\n",
    "            clicksTD = clickmodel(relTD)\n",
    "            clicksP = clickmodel(relP)\n",
    "            for m in range(len(clicksTD)):\n",
    "                lineTD = rankTD[m]\n",
    "                lineP = rankP[m]\n",
    "                if clicksTD[m] == 1 and lineTD[2] == 'E':\n",
    "                    clicksTDE +=1\n",
    "                if clicksTD[m] == 1 and lineTD[2] == 'P':\n",
    "                    clicksTDP +=1\n",
    "                if clicksP[m] == 1 and lineP[2] == 'E':\n",
    "                    clicksPE +=1\n",
    "                if clicksP[m] == 1 and lineP[2] == 'P':\n",
    "                    clicksPP +=1\n",
    "            if clicksTDE>clicksTDP: \n",
    "                winsTDE+=1\n",
    "            if clicksTDE<clicksTDP: \n",
    "                winsTDP+=1 \n",
    "            if clicksPE>clicksPP:   \n",
    "                winsPE+=1 \n",
    "            if clicksPE<clicksPP:   \n",
    "                winsPP+=1 \n",
    "\n",
    "    pTD =  winsTDE/(winsTDE+winsTDP) if  winsTDP+winsTDE>0 else 0.5\n",
    "    pP = winsPE/(winsPE+winsPP) if winsPP+winsPE>0  else 0.5\n",
    "    \n",
    "    # calcutale N\n",
    "    \n",
    "    NTD = sample_size(pTD)\n",
    "    NP = sample_size(pP)\n",
    "    \n",
    "    return NTD,pTD, NP,pP\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\Anaconda2\\envs\\inca362\\lib\\site-packages\\ipykernel_launcher.py:4: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  after removing the cwd from sys.path.\n",
      "C:\\Users\\HP\\Anaconda2\\envs\\inca362\\lib\\site-packages\\ipykernel_launcher.py:5: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random click model team draft table\n",
      "0.05-0.1: min:28.0 median:131.5 max:49832.0\n",
      "0.10-0.15: min:16.0 median:72.0 max:58597.0\n",
      "0.20-0.25: min:14.0 median:318.0 max:52675.0\n",
      "0.25-0.3: min:15.0 median:91.0 max:4390.0\n",
      "0.30-0.35: min:13.0 median:73.0 max:41777.0\n",
      "0.35-0.4: min:21.0 median:206.0 max:49832.0\n",
      "0.40-0.45: min:24.0 median:190.0 max:5886.0\n",
      "0.45-0.5: min:17.0 median:306.0 max:49832.0\n",
      "0.55-0.6: min:28.0 median:227.0 max:9535.0\n",
      "0.60-0.65: min:773.0 median:773.0 max:773.0\n",
      "\n",
      "Position based model probabelistic table\n",
      "0.05-0.1: min:23.0 median:287.0 max:68072.0\n",
      "0.10-0.15: min:21.0 median:278.5 max:49832.0\n",
      "0.20-0.25: min:18.0 median:348.0 max:71388.0\n",
      "0.25-0.3: min:24.0 median:325.0 max:49832.0\n",
      "0.30-0.35: min:24.0 median:191.0 max:55597.0\n",
      "0.35-0.4: min:59.0 median:344.0 max:13569.0\n",
      "0.40-0.45: min:21.0 median:150.0 max:41777.0\n",
      "0.45-0.5: min:17.0 median:101.0 max:47068.0\n",
      "0.55-0.6: min:19.0 median:115.0 max:11464.0\n",
      "0.60-0.65: min:6889.0 median:6889.0 max:6889.0\n",
      "\n",
      "Position based model team draft table\n",
      "0.05-0.1: min:15.0 median:107.0 max:78257.0\n",
      "0.10-0.15: min:13.0 median:18.0 max:32140.0\n",
      "0.20-0.25: min:13.0 median:18.0 max:1444.0\n",
      "0.25-0.3: min:13.0 median:23.5 max:15852.0\n",
      "0.30-0.35: min:13.0 median:18.0 max:2400.0\n",
      "0.35-0.4: min:13.0 median:18.5 max:162.0\n",
      "0.40-0.45: min:13.0 median:13.5 max:44.0\n",
      "0.45-0.5: min:13.0 median:15.0 max:18.0\n",
      "0.55-0.6: min:13.0 median:14.0 max:15.0\n",
      "0.60-0.65: min:13.0 median:13.0 max:13.0\n",
      "\n",
      "Position based model probabilistic table\n",
      "0.05-0.1: min:13.0 median:37.5 max:10801.0\n",
      "0.10-0.15: min:13.0 median:16.0 max:1109.0\n",
      "0.20-0.25: min:13.0 median:64.0 max:108885.0\n",
      "0.25-0.3: min:24.0 median:107.0 max:15071.0\n",
      "0.30-0.35: min:13.0 median:35.0 max:3473.0\n",
      "0.35-0.4: min:15.0 median:54.0 max:985.0\n",
      "0.40-0.45: min:30.0 median:187.5 max:9535.0\n",
      "0.45-0.5: min:13.0 median:16.0 max:141.0\n",
      "0.55-0.6: min:13.0 median:14.0 max:17.0\n",
      "0.60-0.65: min:13.0 median:13.0 max:13.0\n"
     ]
    }
   ],
   "source": [
    "team_table = {}\n",
    "prob_table = {}\n",
    "for score,relevance_list in binned.items():\n",
    "    team_table[score] = []\n",
    "    prob_table[score] = []\n",
    "    for relevance in relevance_list:\n",
    "        permutations = create_possible_permutations(relevance)\n",
    "        for doclist in permutations:\n",
    "            team_sample,pTD,prob_sample,pP = poweranal(doclist,random_click_model)\n",
    "            team_table[score].append(team_sample)\n",
    "            prob_table[score].append(prob_sample)\n",
    "\n",
    "# Random click model\n",
    "print(\"Random click model team draft table\")\n",
    "for key,val in team_table.items():\n",
    "    if not val:continue\n",
    "    l = [v for v in val if not v==float(\"Inf\")]\n",
    "    \n",
    "    print(f'{key-0.05:.2f}-{key}: min:{min(l)} median:{np.median(l)} max:{max(l)}')\n",
    "\n",
    "    \n",
    "print()\n",
    "print(\"Random click model probabilistic table\")\n",
    "for key,val in prob_table.items():\n",
    "    if not val:continue\n",
    "    l = [v for v in val if not v==float(\"Inf\")]    \n",
    "    print(f'{key-0.05:.2f}-{key}: min:{min(l)} median:{np.median(l)} max:{max(l)}')\n",
    "\n",
    "# Position based model    \n",
    "print()  \n",
    "team_table = {}\n",
    "prob_table = {}\n",
    "for score,relevance_list in binned.items():\n",
    "    team_table[score] = []\n",
    "    prob_table[score] = []\n",
    "    for relevance in relevance_list:\n",
    "        permutations = create_possible_permutations(relevance)\n",
    "        for doclist in permutations:\n",
    "            team_sample,pTD,prob_sample,pP = poweranal(doclist)\n",
    "            team_table[score].append(team_sample)\n",
    "            prob_table[score].append(prob_sample)\n",
    "\n",
    "         \n",
    "\n",
    "print(\"Position based model team draft table\")\n",
    "for key,val in team_table.items():\n",
    "    if not val:continue\n",
    "    l = [v for v in val if not v==float(\"Inf\")]\n",
    "    \n",
    "    print(f'{key-0.05:.2f}-{key}: min:{min(l)} median:{np.median(l)} max:{max(l)}')\n",
    "\n",
    "    \n",
    "print()\n",
    "print(\"Position based model probabilistic table\")\n",
    "for key,val in prob_table.items():\n",
    "    if not val:continue\n",
    "    l = [v for v in val if not v==float(\"Inf\")]    \n",
    "    print(f'{key-0.05:.2f}-{key}: min:{min(l)} median:{np.median(l)} max:{max(l)}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Experimental Setup:__\n",
    "\n",
    "This funnel approach gives provides a basis for perfroming analyis to test if algorithm $E$ of a commercial search engine outperforms the production algorithm $P$.\n",
    "The offline testing we performed by ranking the presented documents with a relevance score of 1 or 0.\n",
    "We then calculate the $\\Delta ERR$ and bin the results in $\\Delta ERR$ ranges.\n",
    "Train Parameters alpha and Gamma for Position Based Model on Yandex log file.\n",
    "Run Power Analysis to find minimum sample size. Simulate clicks by generating random number, if number is higher than gamma times epsilon a click is generated. Experiment is run for $k$ times and the  probability of $E$ winning over $P$ is computed.\n",
    "We then run a sanity check using a Random Click Model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of the experiment can be observed in the window above.\n",
    "\n",
    "__Result Analysis:__\n",
    "\n",
    " We observe an inverse correlation between the $\\Delta ERR$ score and the sample size results. The sample size to achieve required significance decreases as the $\\Delta ERR$ increases.\n",
    " Due to the higher $\\Delta ERR$, algorithm $E$ outperforms $P$ with a higher percentage, and thus significance is reached after less impressions.\n",
    " The parameter $\\gamma$ derived in the Position Based Model dominates document selection. The first presented document has a much higher chance of being selected than any other document. If the first document is considered relevant, we will most likely observe a click. The probability of being clicked decreases at every lower position. Because we can only have one document at the first position, that algorithm will have a higher winning chance, however, the starting algorithm is chosen at random for every impression, and thus this advantage should level out. If both models provide relevant documents, it will be harder to discern between the better algorithm, hence the higher number of impressions required.  A higher choice of parameter $\\epsilon$ will lead to a higher sample size since irrelevant documents will achieve a higher click probability. The parameter k determines the number of interleaving experiments we run, if k is set appropriately the win probability should converge to the correct value. We picked k=50 after empirical research.\n",
    "The obtained results show that the required minimum sample size decreases with higher $\\Delta ERR$. \n",
    "\n",
    "The results from the random click model are not of any relevance as they do not take any crucial variables in account (document relevance, ranking) and thus would lead to both algorithms haveing approximately equal performance, hence the high values. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Conclusion and Suggestions:__\n",
    "\n",
    "In this section we suggest certain improvements on the experimental setup for further research.\n",
    "\n",
    "The implemented Position Based Model is but one of many attempts to model user behaviour.\n",
    "Factors like the time spend on each page are not taken into account. The user can find a satisfying answer on the first page and only use the second to confirm. Or the user could click several pages and still not find the needed information. A third scenario is that the user does not click at all but finds the required information in the page summary.\n",
    "These are all variables we could take into account to improve our model to simulate more accurate user behaviour. If we are able to model this behaviour better, we can more accurately determine the better algorithm, which ofcourse is our final goal.\n",
    "\n",
    "In this experiment we did not consider real documents and queries therefore the $\\alpha$ parameter had to be replaced with a generalizing parameter $\\epsilon$. This simplifies the experiment. A more elaborated approach is recommended.\n",
    "\n",
    "One last suggestion for further research is to use a representative set of training-data. Different target groups behave differently and have different expectations and behaviours when using a search engine. Specifying a target group and using specific trainingdata will give more focused information about the performance of the new algorithm.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
