{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information Retrieval   |    Assignment 1-B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import random as rnd\n",
    "import time\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = pd.read_table('YandexRelPredChallenge.txt', delimiter=\"\\t\",index_col=None, \n",
    "                 header=0,names=['SessionID', 'TimePassed','TypeOfAction', 'QueryID','RegionID', 'ListOfURLs', 'URLID',\n",
    "                                 '1','2','3','4','5',\"6\",'7','8'])\n",
    "df_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Open file    \n",
    "fileHandler = open ('YandexRelPredChallenge.txt', \"r\")\n",
    "# Get list of all lines in file\n",
    "listOfLines = fileHandler.readlines()\n",
    "# Close file \n",
    "fileHandler.close()\n",
    "#Empty frame to fill in\n",
    "index=[x for x in range(0,100000)]\n",
    "df = pd.DataFrame(index=index, columns=['SessionID', 'TimePassed',\n",
    "                 'TypeOfAction', 'QueryID','RegionID', 'ListOfURLs', 'URLID'])\n",
    "df.name='data'\n",
    "\n",
    "# Iterate over the lines and fill in the dataframe\n",
    "for ind, line in zip(index, listOfLines):\n",
    "#     print('index')\n",
    "    line = line.strip().split()\n",
    "    if line[2]=='Q':\n",
    "\n",
    "        df.loc[ind] = pd.Series({'SessionID':line[0], 'TimePassed':line[1], 'TypeOfAction':line[2],\n",
    "                                     'QueryID':line[3],'RegionID':line[4], 'ListOfURLs':line[5:], 'URLID':np.nan})\n",
    "    else:\n",
    "        df.loc[ind] = pd.Series({'SessionID':line[0], 'TimePassed':line[1], 'TypeOfAction':line[2],\n",
    "                                     'QueryID':np.nan,'RegionID':np.nan, 'ListOfURLs':np.nan, 'URLID':line[3:]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the types right. \n",
    "df.SessionID = df.SessionID.astype(int)\n",
    "df.TimePassed = df.TimePassed.astype(int)\n",
    "df.TypeOfAction = df.TypeOfAction.astype(str)\n",
    "df.RegionID = df.RegionID.astype(float)\n",
    "df.TimePassed = df.TimePassed.astype(float)\n",
    "def parse_ulrid(x):\n",
    "    if type(x)==list:\n",
    "        return x[0]\n",
    "    else:\n",
    "        return x\n",
    "df.URLID = df.URLID.apply(parse_ulrid)\n",
    "df.URLID = df.URLID.astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create new relevance:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So lets say that our experimental system is what Yandex return as a list now. Production or the system that is going to 'suck' (sort of a dumb system that we are going to compare against) will be constructed as shuffling each ranking for a query. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, other approach, probably the better one: take the initial order as the 'benchmark' create only list of 0,1 (relevnace scores) for each system.\n",
    "    - rember that we are interested only in three [3] first results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create the relevance score ranking (those tables = that we called 'runs')\n",
    "def relevance_list(l,acc=0.5, res_length=3):\n",
    "    \"\"\" Control the 'accuracy'/'performance' with acc param. Only lists of length 3. \"\"\"\n",
    "    if type(l)==list:\n",
    "        res = list()\n",
    "        for x in l:\n",
    "            if rnd.random()<acc:\n",
    "                res.append((x, 1))\n",
    "            else:\n",
    "                res.append((x, 0))\n",
    "        return rnd.sample(res, res_length)\n",
    "    else:\n",
    "        return l\n",
    "\n",
    "df['id_rel_e'] = df.ListOfURLs.apply(relevance_list, acc=0.7) \n",
    "df['id_rel_p'] = df.ListOfURLs.apply(relevance_list) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_rel(l):\n",
    "    if type(l) == list:\n",
    "        res = list()\n",
    "        for tup in l:\n",
    "            res.append(tup[1])\n",
    "        return res\n",
    "    else:\n",
    "        return l\n",
    "def extract_id(l):\n",
    "    if type(l) == list:\n",
    "        res = list()\n",
    "        for tup in l:\n",
    "            res.append(tup[0])\n",
    "        return res\n",
    "    else:\n",
    "        return l\n",
    "df['rel_e'] = df.id_rel_e.apply(extract_rel)\n",
    "df['rel_p'] = df.id_rel_p.apply(extract_rel)\n",
    "df['id_e'] = df.id_rel_e.apply(extract_id)\n",
    "df['id_p'] = df.id_rel_p.apply(extract_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Calculate ERR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "see the formuala for ERR at: http://olivier.chapelle.cc/pub/err.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ERR(l):\n",
    "    \"\"\" List or relevance scores. \"\"\" \n",
    "    if type(l)==list:\n",
    "        theta = [ (2**i - 1)/(2) for i in [0,1]]\n",
    "        err=0\n",
    "        for rank in range(len(l)):\n",
    "            prod = 1\n",
    "            for idx in range(rank):\n",
    "                prod*=(1-theta[l[idx]])\n",
    "            prod*=(1/(rank+1))*theta[l[rank]]\n",
    "            err+=prod\n",
    "        return err\n",
    "    else:\n",
    "        return l\n",
    "df['err_e'] = df.rel_e.apply(ERR)\n",
    "df['err_p'] = df.rel_p.apply(ERR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the deltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test to see if ERR calculation is implemented correctly and if E outperfroms P \n",
    "import math \n",
    "df['diff_e_p'] = df.err_e - df.err_p\n",
    "def count_t(x):\n",
    "    if not math.isnan(x):\n",
    "        if x>0:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0 \n",
    "    else:\n",
    "        return x \n",
    "df['temp_count'] = df.diff_e_p.apply(count_t)\n",
    "df.loc[:,['diff_e_p', 'temp_count']]\n",
    "df.temp_count.value_counts()\n",
    "df.drop(columns=['diff_e_p','temp_count'],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4.1: Estimating the params of click models: Random Click Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13445559411047547\n"
     ]
    }
   ],
   "source": [
    "# Get all the doc ids\n",
    "data = df.drop(columns=['id_rel_e', 'id_rel_p', 'rel_e', 'rel_p', 'id_e',\n",
    "       'id_p', 'err_e', 'err_p'])\n",
    "data.QueryID = data.QueryID.astype(float)\n",
    "list_doc_ids_raw = data.ListOfURLs[df.ListOfURLs.isnull()!=True].values\n",
    "res = list()\n",
    "for i in list_doc_ids_raw:\n",
    "    res += i\n",
    "all_doc_ids = res.copy()\n",
    "# Get all the clicks\n",
    "list_clicks_ids = data.URLID[df.URLID.isnull()!=True].values\n",
    "\n",
    "# rho = probability of a doc to be clicked.\n",
    "rho = len(list_clicks_id)/len(all_doc_ids)\n",
    "print(rho)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4.2: Estimating the params of click models: Posistion-based Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the data for training\n",
    "    - assumption: we assume that all the queries have at least 3 docs shown.\n",
    "    \n",
    "    TODO: check if true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the tables with all the params\n",
    "n_docs = 3\n",
    "unique_queries = data.QueryID[data.QueryID.isnull()!=True].astype(int).unique()\n",
    "n_queries = len(unique_queries) \n",
    "gammas = np.random.rand(n_docs)\n",
    "alphas = np.random.rand(n_docs*n_queries).reshape((n_docs,n_queries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seconds spent 139.9012987613678, mins spent 2.3316883126894634\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# Create query_doc empty pairs\n",
    "query_doc_pairs = defaultdict()\n",
    "for i in unique_queries:\n",
    "    query_doc_pairs[i]=[]\n",
    "    \n",
    "# This code will give an error at the very last session ! -> move last session to the different bit.\n",
    "sessions = data.SessionID.unique()[:-1]\n",
    "for session in sessions:\n",
    "    data_session = data[data.SessionID==session]\n",
    "    click_ids =  data_session.URLID[data_session.URLID.isnull()!=True].values\n",
    "    query_ids = data_session.QueryID[data_session.QueryID.isnull()!=True].values\n",
    "    for query_id in query_ids:\n",
    "        first_3_docs = data_session.ListOfURLs[data.QueryID==query_id].iloc[0]\n",
    "        first_3_docs = np.array(list(map(int,first_3_docs))[:3])\n",
    "        # take index in the current data_session\n",
    "        query_id_ind = data_session.QueryID[data_session.QueryID==query_id].index[0]\n",
    "        flag = not math.isnan(data.URLID.iloc[query_id_ind+1])\n",
    "        # if the next entry is click\n",
    "        if flag:\n",
    "            click_log = np.array([0,0,0])\n",
    "            query_id_ind+=1\n",
    "            while flag==1:\n",
    "                click_urlid = data.URLID.iloc[query_id_ind]\n",
    "                # if click_urlid in first_3_docs then click is 1 on that position.\n",
    "                doc_u_id = np.where(first_3_docs==click_urlid)\n",
    "                click_log[doc_u_id]=1\n",
    "                # increase the count\n",
    "                query_id_ind +=1\n",
    "                flag = not math.isnan(data.URLID.iloc[query_id_ind])\n",
    "            # do updates\n",
    "        else:\n",
    "            click_log = np.array([0,0,0])\n",
    "            # do updates\n",
    "            \n",
    "        # Save query_id-click_log (=session-doc) pair for each query_id\n",
    "        query_doc_pairs[query_id].append(click_log)\n",
    "    \n",
    "end = time.time()\n",
    "print(f'suka bleyat : seconds spent {end - start}, mins spent {(end - start)/60}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "query_doc_pairs_old = query_doc_pairs.copy()\n",
    "# query_doc_pairs_old"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform EM steps (update assignments ; update params):\n",
    "    \n",
    "    1) initialize alpha and gamma at random \n",
    "    2) For each time step:\n",
    "        - Expecation: do not really have to do this step [?], what we want to see at the end: P(C|u) \n",
    "            - calculate P(C|u) for each u (u is from 1 to 3 = positions in the shown list)\n",
    "            - \" P(C|u) = P(A_uq|E,u)P(E|u) \"\n",
    "                \n",
    "        - Maximization: update alpha and gamma params\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alphas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert unique_queries.shape[0] == alphas.shape[1], 'Error, bleayt'\n",
    "# for comparison :\n",
    "alphas_old = alphas.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 625,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seconds spent 1.3521900177001953, mins spent 0.022536500295003255\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# Do the updates for alphas:\n",
    "\n",
    "# refer to alphas array with query_num as that it is how the are strored there:\n",
    "for query_num, query_id in enumerate(query_doc_pairs):\n",
    "    for doc_u in range(n_docs):\n",
    "        factor_1 = 1/len(query_doc_pairs[query_id])\n",
    "        summ =  0\n",
    "        for sessions_docs in query_doc_pairs[query_id]:\n",
    "            click = sessions_docs[doc_u]\n",
    "            summ += click + (1-click)*( (1-gammas[doc_u])*alphas[doc_u][query_num] ) / \\\n",
    "                                        (1-gammas[doc_u]*alphas[doc_u][query_num])\n",
    "        alphas[doc_u][query_num] = factor_1*summ\n",
    "        \n",
    "end = time.time()\n",
    "print(f'seconds spent {end - start}, mins spent {(end - start)/60}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 627,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# is that true that we ve changed every alpha ?\n",
    "not (alphas_old == alphas).any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gammas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.64732249, 0.79368901, 0.802856  ])"
      ]
     },
     "execution_count": 629,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gammas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "# Do the updates for gammas:\n",
    "\n",
    "# refer to alphas array with query_num as that it is how the are strored there:\n",
    "for query_num, query_id in enumerate(query_doc_pairs):\n",
    "    for doc_u in range(n_docs):\n",
    "        factor_1 = 1 / len(query_doc_pairs[query_id])\n",
    "        summ =  0\n",
    "        for sessions_docs in query_doc_pairs[query_id]:\n",
    "            click = sessions_docs[doc_u]\n",
    "            summ += click + (1-click)*( (1-gammas[doc_u])*alphas[doc_u][query_num] ) / \\\n",
    "                                        (1-gammas[doc_u]*alphas[doc_u][query_num])\n",
    "        alphas[doc_u][query_num] = factor_1*summ\n",
    "        \n",
    "end = time.time()\n",
    "print(f'seconds spent {end - start}, mins spent {(end - start)/60}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
