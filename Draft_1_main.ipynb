{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information Retrieval   |    Assignment 1-B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import random as rnd\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from functools import reduce\n",
    "import math\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Open file    \n",
    "fileHandler = open ('YandexRelPredChallenge.txt', \"r\")\n",
    "# Get list of all lines in file\n",
    "listOfLines = fileHandler.readlines()\n",
    "# Close file \n",
    "fileHandler.close()\n",
    "#Empty frame to fill in\n",
    "index=[x for x in range(0,100000)]\n",
    "df = pd.DataFrame(index=index, columns=['SessionID', 'TimePassed',\n",
    "                 'TypeOfAction', 'QueryID','RegionID', 'ListOfURLs', 'URLID'])\n",
    "df.name='data'\n",
    "\n",
    "# Iterate over the lines and fill in the dataframe\n",
    "for ind, line in zip(index, listOfLines):\n",
    "#     print('index')\n",
    "    line = line.strip().split()\n",
    "    if line[2]=='Q':\n",
    "\n",
    "        df.loc[ind] = pd.Series({'SessionID':line[0], 'TimePassed':line[1], 'TypeOfAction':line[2],\n",
    "                                     'QueryID':line[3],'RegionID':line[4], 'ListOfURLs':line[5:], 'URLID':np.nan})\n",
    "    else:\n",
    "        df.loc[ind] = pd.Series({'SessionID':line[0], 'TimePassed':line[1], 'TypeOfAction':line[2],\n",
    "                                     'QueryID':np.nan,'RegionID':np.nan, 'ListOfURLs':np.nan, 'URLID':line[3:]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the types right. \n",
    "df.SessionID = df.SessionID.astype(int)\n",
    "df.TimePassed = df.TimePassed.astype(int)\n",
    "df.TypeOfAction = df.TypeOfAction.astype(str)\n",
    "df.RegionID = df.RegionID.astype(float)\n",
    "df.TimePassed = df.TimePassed.astype(float)\n",
    "def parse_ulrid(x):\n",
    "    if type(x)==list:\n",
    "        return x[0]\n",
    "    else:\n",
    "        return x\n",
    "df.URLID = df.URLID.apply(parse_ulrid)\n",
    "df.URLID = df.URLID.astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create new relevance:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So lets say that our experimental system is what Yandex return as a list now. Production or the system that is going to 'suck' (sort of a dumb system that we are going to compare against) will be constructed as shuffling each ranking for a query. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, other approach, probably the better one: take the initial order as the 'benchmark' create only list of 0,1 (relevnace scores) for each system.\n",
    "    - rember that we are interested only in three [3] first results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'itertools' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-4e52fc4a49b2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdoc_length\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mrelevance_pairs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mseq\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mitertools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mproduct\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"01\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrepeat\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mrelevance_pairs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrelevance_pairs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'itertools' is not defined"
     ]
    }
   ],
   "source": [
    "# Create new relevance scores \n",
    "relevances = [0,1]\n",
    "doc_length = 3 \n",
    "\n",
    "relevance_pairs=[\"\".join(seq) for seq in itertools.product(\"01\", repeat=6)]\n",
    "relevance_pairs = [(p[:3],p[3:])for p in relevance_pairs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Calculate ERR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ERR(l):\n",
    "    theta = [ (2**i - 1)/(2) for i in [0,1]]\n",
    "    err=0\n",
    "    \n",
    "    for rank in range(len(l)):\n",
    "        prod = 1\n",
    "        for idx in range(rank):\n",
    "            prod*=(1-theta[int(l[idx])])\n",
    "        prod*=(1/(rank+1))*theta[int(l[rank])]\n",
    "        err+=prod\n",
    "    return err\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "see the formuala for ERR at: http://olivier.chapelle.cc/pub/err.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the deltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'relevance_pairs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-7528e3ee46a5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mbinned\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m}\u001b[0m\u001b[1;31m#all below 0.05 is discarded The numbers are the upperbound\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mtup\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrelevance_pairs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0md_err\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mERR\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mERR\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0md_err\u001b[0m\u001b[1;33m<\u001b[0m\u001b[1;36m0.05\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[1;32mcontinue\u001b[0m \u001b[1;31m#discard this entry\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'relevance_pairs' is not defined"
     ]
    }
   ],
   "source": [
    "binned = { x/100:[]for x in range(10,100,5) }#all below 0.05 is discarded The numbers are the upperbound\n",
    "for tup in relevance_pairs:\n",
    "    d_err = ERR(tup[0]) - ERR(tup[1])\n",
    "    if d_err<0.05:\n",
    "        continue #discard this entry\n",
    "    \n",
    "    d_err =int(5 * round((d_err*100)/5))/100\n",
    "    binned[d_err].append(tup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_possible_permutations(tup):\n",
    "    ids=['a','b','c','d','e','f']\n",
    "    pair1 = ['a','b','c']\n",
    "    rel1  = tup[0]\n",
    "    rel2  = tup[1]\n",
    "    possible_pairs = []\n",
    "    \n",
    "    possible_ids = []\n",
    "    for idx in range(3):\n",
    "        possible_ids.append([ids[idx+3]])\n",
    "        for _id,r1 in enumerate(rel1):\n",
    "                if r1==rel2[idx]:\n",
    "                    possible_ids[idx].append(ids[_id])\n",
    "                    \n",
    "    for l0 in possible_ids[0]:\n",
    "        for l1 in possible_ids[1]:\n",
    "            for l2 in possible_ids[2]:\n",
    "                if l0 is not l1 is not l2 is not l0:\n",
    "                    possible_pairs.append([l0,l1,l2])\n",
    "    pairs = []\n",
    "    for p in possible_pairs:\n",
    "        pairs.append((pair1,p))\n",
    "    return pairs\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def team_draft(tup,start=0,):\n",
    "    l = []\n",
    "    tup = (list(tup[0]),list(tup[1]))\n",
    "    for idx in range(6):\n",
    "        start+=1\n",
    "        if len(tup[start%2])==0:continue\n",
    "        elem = tup[start%2].pop(0)\n",
    "        tup = (list(filter(lambda a: a != elem, tup[0])), list(filter(lambda a: a != elem, tup[1])))\n",
    "        l.append((elem,start%2))\n",
    "    return l\n",
    "\n",
    "def probabelistic_interleaving(tup):\n",
    "    l = []\n",
    "    tup = (list(tup[0]),list(tup[1]))\n",
    "\n",
    "    for x in range(6):\n",
    "        active=[]\n",
    "        if(len(tup[0]))>0:active+=[0]\n",
    "        if(len(tup[1]))>0:active+=[1]    \n",
    "        if not active:break\n",
    "        cur = np.random.choice(active)#get the currently selected model\n",
    "        \n",
    "        unnormalized_probs=[]\n",
    "        for idx in range(len(tup[cur])):\n",
    "            unnormalized_probs.append(1/((idx+1)**3))#tau is 3 from the paper\n",
    "        unnormalized_probs = [x/sum(unnormalized_probs) for x in unnormalized_probs]\n",
    "        elem = np.random.choice(tup[cur],p=unnormalized_probs)\n",
    "        tup = (list(filter(lambda a: a != elem, tup[0])), list(filter(lambda a: a != elem, tup[1])))\n",
    "        l.append((elem,cur))\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('d', 1), ('a', 0), ('b', 0), ('c', 1), ('f', 1)]\n",
      "[('d', 1), ('a', 0), ('c', 1), ('b', 0), ('f', 1)]\n",
      "(['a', 'b', 'c'], ['d', 'c', 'f'])\n"
     ]
    }
   ],
   "source": [
    "l = create_possible_permutations(['111','111'])\n",
    "print(probabelistic_interleaving(l[10]))\n",
    "print(team_draft(l[10]))\n",
    "\n",
    "print(l[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4.1: Estimating the params of click models: Random Click Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the doc ids\n",
    "data = df.drop(columns=['id_rel_e', 'id_rel_p', 'rel_e', 'rel_p', 'id_e',\n",
    "       'id_p', 'err_e', 'err_p'])\n",
    "data.QueryID = data.QueryID.astype(float)\n",
    "list_doc_ids_raw = data.ListOfURLs[df.ListOfURLs.isnull()!=True].values\n",
    "res = list()\n",
    "for i in list_doc_ids_raw:\n",
    "    res += i\n",
    "all_doc_ids = res.copy()\n",
    "# Get all the clicks\n",
    "list_clicks_ids = data.URLID[df.URLID.isnull()!=True].values\n",
    "\n",
    "# rho = probability of a doc to be clicked.\n",
    "rho = len(list_clicks_id)/len(all_doc_ids)\n",
    "print(rho)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4.2: Estimating the params of click models: Posistion-based Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the data for training\n",
    "    - assumption: we assume that all the queries have at least 3 docs shown.\n",
    "    \n",
    "    TODO: check if true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.copy()\n",
    "data.QueryID = data.QueryID.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the tables with all the params\n",
    "n_docs = 6\n",
    "unique_queries = data.QueryID[data.QueryID.isnull()!=True].astype(int).unique()\n",
    "n_queries = len(unique_queries) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "# Create query_doc empty pairs\n",
    "query_doc_pairs = defaultdict()\n",
    "for i in unique_queries:\n",
    "    query_doc_pairs[i]=[]\n",
    "    \n",
    "# This code will give an error at the very last session ! -> move last session to the different bit.\n",
    "sessions = data.SessionID.unique()[:-1]\n",
    "for session in sessions:\n",
    "    data_session = data[data.SessionID==session]\n",
    "    click_ids =  data_session.URLID[data_session.URLID.isnull()!=True].values\n",
    "    query_ids = data_session.QueryID[data_session.QueryID.isnull()!=True].values\n",
    "    for query_id in query_ids:\n",
    "        first_n_docs = data_session.ListOfURLs[data.QueryID==query_id].iloc[0]\n",
    "        first_n_docs = np.array(list(map(int,first_n_docs))[:n_docs])\n",
    "        # take index in the current data_session\n",
    "        query_id_ind = data_session.QueryID[data_session.QueryID==query_id].index[0]\n",
    "        flag = not math.isnan(data.URLID.iloc[query_id_ind+1])\n",
    "        # if the next entry is click\n",
    "        if flag:\n",
    "            click_log = np.zeros(n_docs)\n",
    "            query_id_ind+=1\n",
    "            while flag==1:\n",
    "                click_urlid = data.URLID.iloc[query_id_ind]\n",
    "                # if click_urlid in first_n_docs then click is 1 on that position.\n",
    "                doc_u_id = np.where(first_n_docs==click_urlid)\n",
    "                click_log[doc_u_id]=1\n",
    "                # increase the count\n",
    "                query_id_ind +=1\n",
    "                flag = not math.isnan(data.URLID.iloc[query_id_ind])\n",
    "            # do updates\n",
    "        else:\n",
    "            click_log = np.zeros(n_docs)\n",
    "            # do updates\n",
    "            \n",
    "        # Save query_id-click_log (=session-doc) pair for each query_id\n",
    "        query_doc_pairs[query_id].append(click_log)\n",
    "    \n",
    "end = time.time()\n",
    "print(f'suka bleyat, but still kinda quick tho : seconds spent {end - start}, mins spent {(end - start)/60}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "query_doc_pairs_old = query_doc_pairs.copy()\n",
    "query_doc_pairs_old"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform EM steps (update assignments ; update params):\n",
    "    \n",
    "    1) initialize alpha and gamma at random \n",
    "    2) For each time step:\n",
    "        - Expecation: do not really have to do this step [?], what we want to see at the end: P(C|u) \n",
    "            - calculate P(C|u) for each u (u is from 1 to 3 = positions in the shown list)\n",
    "            - \" P(C|u) = P(A_uq|E,u)P(E|u) \"\n",
    "                \n",
    "        - Maximization: update alpha and gamma params\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = np.random.rand(n_docs*n_queries).reshape((n_docs, n_queries))\n",
    "gammas = np.random.rand(n_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alphas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert unique_queries.shape[0] == alphas.shape[1], 'Error, bleayt'\n",
    "\n",
    "# for comparison :\n",
    "alphas_old = alphas.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "# Do the updates for alphas:\n",
    "\n",
    "def train_alphas(alphas, gammas, query_doc_pairs, n_docs):\n",
    "    # refer to alphas array with query_num as that it is how the are strored there:\n",
    "    for query_num, query_id in enumerate(query_doc_pairs):\n",
    "        for doc_u in range(n_docs):\n",
    "            factor_1 = 1/len(query_doc_pairs[query_id])\n",
    "            summ =  0\n",
    "            for sessions_docs in query_doc_pairs[query_id]:\n",
    "                click = sessions_docs[doc_u]\n",
    "                summ += click + (1-click)*( (1-gammas[doc_u])*alphas[doc_u][query_num] ) / \\\n",
    "                                            (1-gammas[doc_u]*alphas[doc_u][query_num])\n",
    "            alphas[doc_u][query_num] = factor_1*summ\n",
    "    return alphas\n",
    "\n",
    "alphas = train_alphas(alphas, gammas, query_doc_pairs, n_docs)\n",
    "\n",
    "end = time.time()\n",
    "print(f'seconds spent {end - start}, mins spent {(end - start)/60}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is that true that we ve changed every alpha ?\n",
    "print('Is that true that we ve changed every alpha ? ->', not (alphas_old == alphas).any())\n",
    "assert alphas_old.shape == alphas.shape, 'new shapes are fucked up'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gammas\n",
    "\n",
    "We normalize by all the sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sess = [ query_doc_pairs[idd] for idd in query_doc_pairs]\n",
    "all_sess = reduce(lambda x,y: x+y, all_sess)\n",
    "\n",
    "# for comparison :\n",
    "gammas_old = gammas.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "# Do the updates for gammas:\n",
    "\n",
    "def train_gammas(alphas, gammas, query_doc_pairs, n_docs):\n",
    "    for doc_u in range(n_docs):\n",
    "        # refer to alphas array with query_num as that it is how the are strored there:\n",
    "        summ =  0\n",
    "        for query_num, query_id in enumerate(query_doc_pairs):\n",
    "            factor_1 = 1 / len(all_sess)\n",
    "            for sessions_docs in query_doc_pairs[query_id]:\n",
    "                click = sessions_docs[doc_u]\n",
    "                summ += click + (1-click)*( (1-alphas[doc_u][query_num])*gammas[doc_u] ) / \\\n",
    "                                            (1-gammas[doc_u]*alphas[doc_u][query_num])        \n",
    "        gammas[doc_u] = factor_1 * summ\n",
    "    \n",
    "    return gammas\n",
    "\n",
    "gammas = train_gammas(alphas, gammas, query_doc_pairs, n_docs)\n",
    "\n",
    "end = time.time()\n",
    "print(f'seconds spent {end - start}, mins spent {(end - start)/60}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test to see if we have all queries filled in\n",
    "def check_sanity(l):\n",
    "    if l==[]:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "test = pd.Series(query_doc_pairs)\n",
    "test_res = test.apply(check_sanity)\n",
    "print('should be all true, coz otherwise we have queries with no sessions', np.unique(np.array(test_res)))\n",
    "\n",
    "assert gammas_old.shape == gammas.shape, 'shapes are fucked'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gammas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training multiple times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gammas = np.random.rand(n_docs)\n",
    "alphas = np.random.rand(n_docs*n_queries).reshape((n_docs,n_queries))\n",
    "\n",
    "#  Training till convergence: \n",
    "n_epochs = 100\n",
    "diff_alphas = [1]\n",
    "diff_gammas = [1]\n",
    "\n",
    "for i in range(n_epochs):\n",
    "    gammas = train_gammas(alphas, gammas, query_doc_pairs, n_docs)\n",
    "    alphas = train_alphas(alphas, gammas, query_doc_pairs, n_docs)\n",
    "    \n",
    "    #For plotting: checking convergence\n",
    "    diff_alphas.append( diff_alphas[-1] - sum(sum(alphas)) )\n",
    "    diff_gammas.append( diff_gammas[-1] - sum(gammas) )\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = np.linspace(0, n_epochs+1, n_epochs+1)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7,5))\n",
    "ax.plot(x,diff_gammas,marker='x')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gammas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5\n",
    "\n",
    "Power analysis:\n",
    "interleave and simulate clicks on every permutation for $k$ times. Then calculate winning prbability $p$. Do the Power Analysis to find minimum number of sample size $N$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables\n",
    "\n",
    "\n",
    "k = 50\n",
    "rnd.seed(21)\n",
    "gammas = [0.484221138516365, 0.21930976273093877, 0.1581402982275157]\n",
    "epsilon = 0.005\n",
    "\n",
    "p0 = 0.5\n",
    "alphaN = 0.05\n",
    "betaN = 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if the probability of the PBM is higher than a ranomly generates number we assume a click happens\n",
    "\n",
    "def sim_click(rankList):\n",
    "    pList = np.multiply(np.ones(len(rankList)),gammas)\n",
    "    for n in range(len(pList)):\n",
    "        if rankList(n)==1:\n",
    "            pList(n)*= (1-epsilon)\n",
    "        else:\n",
    "            pList(n)*= epsilon\n",
    "    randP = rnd.uniform(size=len(pList))\n",
    "    click = np.zeros(len(pList))\n",
    "    for n in range(len(pList)):\n",
    "        if pList(n) - randP(n) > 0:\n",
    "            click(n) = 1\n",
    "    return click\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_size(p1):\n",
    "    delta = np.absolute(p1-p0)\n",
    "    za = scipy.stats.zscore(1-alphaN)\n",
    "    zb = scipy.stats.zscore(1-betaN)\n",
    "    N1 = np.ceil(np.frac(za*np.srq(p0*(1-p0))+zb*np.sqr(p1*(1-p1)),delta)^2)\n",
    "    N = N1 + np.frac(1,delta)\n",
    "    \n",
    "    return N\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poweranal(tub):\n",
    "    winsTDE = 0\n",
    "    winsPE = 0\n",
    "    winsTDP = 0\n",
    "    winsPP = 0\n",
    "    \n",
    "    for n in range(k):\n",
    "        rankTD = teamdraft(tub)\n",
    "        rankP = probabelistic_interleaving(tub)\n",
    "        clicksTD = sim_clicks(rankTD[:,0])\n",
    "        clicksP = sim_clicks(rankP[:,0])\n",
    "        clicksTDE = 0\n",
    "        clicksTDP = 0\n",
    "        clicksPE = 0\n",
    "        clicksPP = 0\n",
    "        for m in range(len(clicksTD)):\n",
    "            if clicksTD[m] == 1 & rankTD[1,m] == 'E':\n",
    "                clicksTDE +=1\n",
    "            if clicksTD[m] == 1 & rankTD[1,m] == 'P':\n",
    "                clicksTDP +=1\n",
    "            if clicksP[m] == 1 & rankP[1,m] == 'E':\n",
    "                clicksPE +=1\n",
    "            if clicksP[m] == 1 & rankP[1,m] == 'P':\n",
    "                clicksPP +=1\n",
    "        [winsTDE+=1 if clicksTDE>clicksTDP]\n",
    "        [winsTDP+=1 if clicksTDE<clicksTDP]\n",
    "        [winsPE+=1 if clicksPE>clicksPP]\n",
    "        [winsPE+=1 if clicksPE<clicksPP]\n",
    "        \n",
    "    pTD = winsTDE/(winsTDE+winsTDP)\n",
    "    pP = winsPE/(winsPE+winsPP)\n",
    "    \n",
    "    # calcutale N\n",
    "    \n",
    "    NTD = sample_size(pTD)\n",
    "    NP = sample_size(pP)\n",
    "    \n",
    "    return NTD, NP\n",
    "    \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Power Analysis for all permutations AUB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "tuple indices must be integers or slices, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-4597004cfd85>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtest1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'E'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'P'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'E'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'P'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'E'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'P'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mtest1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: tuple indices must be integers or slices, not tuple"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
