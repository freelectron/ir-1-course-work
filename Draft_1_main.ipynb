{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information Retrieval   |    Assignment 1-B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import random as rnd\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from functools import reduce\n",
    "import math\n",
    "from scipy import stats\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Open file    \n",
    "fileHandler = open ('YandexRelPredChallenge.txt', \"r\")\n",
    "# Get list of all lines in file\n",
    "listOfLines = fileHandler.readlines()\n",
    "# Close file \n",
    "fileHandler.close()\n",
    "#Empty frame to fill in\n",
    "index=[x for x in range(0,100000)]\n",
    "df = pd.DataFrame(index=index, columns=['SessionID', 'TimePassed',\n",
    "                 'TypeOfAction', 'QueryID','RegionID', 'ListOfURLs', 'URLID'])\n",
    "df.name='data'\n",
    "\n",
    "# Iterate over the lines and fill in the dataframe\n",
    "for ind, line in zip(index, listOfLines):\n",
    "#     print('index')\n",
    "    line = line.strip().split()\n",
    "    if line[2]=='Q':\n",
    "\n",
    "        df.loc[ind] = pd.Series({'SessionID':line[0], 'TimePassed':line[1], 'TypeOfAction':line[2],\n",
    "                                     'QueryID':line[3],'RegionID':line[4], 'ListOfURLs':line[5:], 'URLID':np.nan})\n",
    "    else:\n",
    "        df.loc[ind] = pd.Series({'SessionID':line[0], 'TimePassed':line[1], 'TypeOfAction':line[2],\n",
    "                                     'QueryID':np.nan,'RegionID':np.nan, 'ListOfURLs':np.nan, 'URLID':line[3:]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the types right. \n",
    "df.SessionID = df.SessionID.astype(int)\n",
    "df.TimePassed = df.TimePassed.astype(int)\n",
    "df.TypeOfAction = df.TypeOfAction.astype(str)\n",
    "df.RegionID = df.RegionID.astype(float)\n",
    "df.TimePassed = df.TimePassed.astype(float)\n",
    "def parse_ulrid(x):\n",
    "    if type(x)==list:\n",
    "        return x[0]\n",
    "    else:\n",
    "        return x\n",
    "df.URLID = df.URLID.apply(parse_ulrid)\n",
    "df.URLID = df.URLID.astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create new relevance:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So lets say that our experimental system is what Yandex return as a list now. Production or the system that is going to 'suck' (sort of a dumb system that we are going to compare against) will be constructed as shuffling each ranking for a query. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, other approach, probably the better one: take the initial order as the 'benchmark' create only list of 0,1 (relevnace scores) for each system.\n",
    "    - remember that we are interested only in three [3] first results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new relevance scores \n",
    "relevances = [0,1]\n",
    "doc_length = 3 \n",
    "MODELS =[\"E\",\"P\"]\n",
    "relevance_pairs=[\"\".join(seq) for seq in itertools.product(\"01\", repeat=6)]\n",
    "relevance_pairs = [(p[:3],p[3:])for p in relevance_pairs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Calculate ERR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ERR(l):\n",
    "    theta = [ (2**i - 1)/(2) for i in [0,1]]\n",
    "    err=0\n",
    "    \n",
    "    for rank in range(len(l)):\n",
    "        prod = 1\n",
    "        for idx in range(rank):\n",
    "            prod*=(1-theta[int(l[idx])])\n",
    "        prod*=(1/(rank+1))*theta[int(l[rank])]\n",
    "        err+=prod\n",
    "    return err\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "see the formuala for ERR at: http://olivier.chapelle.cc/pub/err.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the deltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "binned = { x/100:[]for x in range(10,100,5) }#all below 0.05 is discarded The numbers are the upperbound\n",
    "for tup in relevance_pairs:\n",
    "    d_err = ERR(tup[0]) - ERR(tup[1])\n",
    "    if d_err<0.05:\n",
    "        continue #discard this entry\n",
    "    \n",
    "    d_err =int(5 * round((d_err*100)/5))/100\n",
    "    binned[d_err].append(tup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_possible_permutations(tup):\n",
    "    ids=['a','b','c','d','e','f']\n",
    "    pair1 = ['a','b','c']\n",
    "    rel1  = tup[0]\n",
    "    rel2  = tup[1]\n",
    "    possible_pairs = []\n",
    "    \n",
    "    possible_ids = []\n",
    "    for idx in range(3):\n",
    "        possible_ids.append([(ids[idx+3],rel2[idx])])\n",
    "        for _id,r1 in enumerate(rel1):\n",
    "                if r1==rel2[idx]:\n",
    "                    possible_ids[idx].append((ids[_id],rel2[idx]))#find all possible overlapping results\n",
    "    \n",
    "    for l0 in possible_ids[0]:\n",
    "        for l1 in possible_ids[1]:\n",
    "            for l2 in possible_ids[2]:\n",
    "                if l0 is not l1 is not l2 is not l0:\n",
    "                    possible_pairs.append([l0,l1,l2])\n",
    "    pairs = []\n",
    "    for p in possible_pairs:\n",
    "        pairs.append(([ tup for tup in zip(pair1,rel1)],p))\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def team_draft(tup,start=0,):\n",
    "    l = []\n",
    "    tup = (list(tup[0]),list(tup[1]))\n",
    "    for idx in range(6):\n",
    "        start+=1\n",
    "        if len(tup[start%2])==0:continue\n",
    "        elem = tup[start%2].pop(0)\n",
    "        tup = (list(filter(lambda a: a != elem, tup[0])), list(filter(lambda a: a != elem, tup[1])))\n",
    "        l.append((elem +(MODELS[start%2],)))\n",
    "    return l\n",
    "\n",
    "def probabelistic_interleaving(tup):\n",
    "    l = []\n",
    "    tup = (list(tup[0]),list(tup[1]))\n",
    "\n",
    "    for x in range(6):\n",
    "        active=[]\n",
    "        if(len(tup[0]))>0:active+=[0]\n",
    "        if(len(tup[1]))>0:active+=[1]    \n",
    "        if not active:break\n",
    "        cur = np.random.choice(active)#get the currently selected model\n",
    "        \n",
    "        unnormalized_probs=[]\n",
    "        for idx in range(len(tup[cur])):\n",
    "            unnormalized_probs.append(1/((idx+1)**3))#tau is 3 from the paper\n",
    "        unnormalized_probs = [x/sum(unnormalized_probs) for x in unnormalized_probs]\n",
    "        elemz_idx = np.random.choice(range(len(tup[cur])),p=unnormalized_probs)\n",
    "        elem = tup[cur][idx]\n",
    "        \n",
    "        tup = (list(filter(lambda a: a != elem, tup[0])), list(filter(lambda a: a != elem, tup[1])))\n",
    "        l.append((elem + (MODELS[cur],)))\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('b', '1', 'P'), ('c', '1', 'E'), ('d', '1', 'P'), ('a', '1', 'E')]\n",
      "[('d', '1', 'P'), ('a', '1', 'E'), ('b', '1', 'P'), ('c', '1', 'E')]\n"
     ]
    }
   ],
   "source": [
    "l = create_possible_permutations(['111','111'])\n",
    "print(probabelistic_interleaving(l[10]))\n",
    "print(team_draft(l[10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4.1: Estimating the params of click models: Random Click Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13445559411047547\n"
     ]
    }
   ],
   "source": [
    "# Get all the doc ids\n",
    "data = df.copy() #drop(columns=['id_rel_e', 'id_rel_p', 'rel_e', 'rel_p', 'id_e',\n",
    "#        'id_p', 'err_e', 'err_p'])\n",
    "data.QueryID = data.QueryID.astype(float)\n",
    "list_doc_ids_raw = data.ListOfURLs[df.ListOfURLs.isnull()!=True].values\n",
    "res = list()\n",
    "for i in list_doc_ids_raw:\n",
    "    res += i\n",
    "all_doc_ids = res.copy()\n",
    "# Get all the clicks\n",
    "list_clicks_ids = data.URLID[df.URLID.isnull()!=True].values\n",
    "\n",
    "# rho = probability of a doc to be clicked.\n",
    "rho = len(list_clicks_ids)/len(all_doc_ids)\n",
    "print(rho)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4.2: Estimating the params of click models: Posistion-based Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the data for training\n",
    "    - assumption: we assume that all the queries have at least 3 docs shown.\n",
    "    \n",
    "    TODO: check if true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.copy()\n",
    "data.QueryID = data.QueryID.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the tables with all the params\n",
    "n_docs = 6\n",
    "unique_queries = data.QueryID[data.QueryID.isnull()!=True].astype(int).unique()\n",
    "n_queries = len(unique_queries) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-90-e035dae6f01e>, line 41)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-90-e035dae6f01e>\"\u001b[0;36m, line \u001b[0;32m41\u001b[0m\n\u001b[0;31m    print(f', but still kinda quick tho : seconds spent {end - start}, mins spent {(end - start)/60}')\u001b[0m\n\u001b[0m                                                                                                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# Create query_doc empty pairs\n",
    "query_doc_pairs = defaultdict()\n",
    "for i in unique_queries:\n",
    "    query_doc_pairs[i]=[]\n",
    "    \n",
    "# This code will give an error at the very last session ! -> move last session to the different bit.\n",
    "sessions = data.SessionID.unique()[:-1]\n",
    "for session in sessions:\n",
    "    data_session = data[data.SessionID==session]\n",
    "    click_ids =  data_session.URLID[data_session.URLID.isnull()!=True].values\n",
    "    query_ids = data_session.QueryID[data_session.QueryID.isnull()!=True].values\n",
    "    for query_id in query_ids:\n",
    "        first_n_docs = data_session.ListOfURLs[data.QueryID==query_id].iloc[0]\n",
    "        first_n_docs = np.array(list(map(int,first_n_docs))[:n_docs])\n",
    "        # take index in the current data_session\n",
    "        query_id_ind = data_session.QueryID[data_session.QueryID==query_id].index[0]\n",
    "        flag = not math.isnan(data.URLID.iloc[query_id_ind+1])\n",
    "        # if the next entry is click\n",
    "        if flag:\n",
    "            click_log = np.zeros(n_docs)\n",
    "            query_id_ind+=1\n",
    "            while flag==1:\n",
    "                click_urlid = data.URLID.iloc[query_id_ind]\n",
    "                # if click_urlid in first_n_docs then click is 1 on that position.\n",
    "                doc_u_id = np.where(first_n_docs==click_urlid)\n",
    "                click_log[doc_u_id]=1\n",
    "                # increase the count\n",
    "                query_id_ind +=1\n",
    "                flag = not math.isnan(data.URLID.iloc[query_id_ind])\n",
    "            # do updates\n",
    "        else:\n",
    "            click_log = np.zeros(n_docs)\n",
    "            # do updates\n",
    "            \n",
    "        # Save query_id-click_log (=session-doc) pair for each query_id\n",
    "        query_doc_pairs[query_id].append(click_log)\n",
    "    \n",
    "end = time.time()\n",
    "print(f', but still kinda quick tho : seconds spent {end - start}, mins spent {(end - start)/60}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'query_doc_pairs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-91-ba00337ad248>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mquery_doc_pairs_old\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery_doc_pairs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mquery_doc_pairs_old\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'query_doc_pairs' is not defined"
     ]
    }
   ],
   "source": [
    "query_doc_pairs_old = query_doc_pairs.copy()\n",
    "query_doc_pairs_old"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform EM steps (update assignments ; update params):\n",
    "    \n",
    "    1) initialize alpha and gamma at random \n",
    "    2) For each time step:\n",
    "        - Expecation: do not really have to do this step [?], what we want to see at the end: P(C|u) \n",
    "            - calculate P(C|u) for each u (u is from 1 to 3 = positions in the shown list)\n",
    "            - \" P(C|u) = P(A_uq|E,u)P(E|u) \"\n",
    "                \n",
    "        - Maximization: update alpha and gamma params\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = np.random.rand(n_docs*n_queries).reshape((n_docs, n_queries))\n",
    "gammas = np.random.rand(n_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alphas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'alphas' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-92-d91421411795>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0munique_queries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0malphas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Error, bleayt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# for comparison :\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0malphas_old\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malphas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'alphas' is not defined"
     ]
    }
   ],
   "source": [
    "assert unique_queries.shape[0] == alphas.shape[1], 'Error, bleayt'\n",
    "\n",
    "# for comparison :\n",
    "alphas_old = alphas.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "# Do the updates for alphas:\n",
    "\n",
    "def train_alphas(alphas, gammas, query_doc_pairs, n_docs):\n",
    "    # refer to alphas array with query_num as that it is how the are strored there:\n",
    "    for query_num, query_id in enumerate(query_doc_pairs):\n",
    "        for doc_u in range(n_docs):\n",
    "            factor_1 = 1/len(query_doc_pairs[query_id])\n",
    "            summ =  0\n",
    "            for sessions_docs in query_doc_pairs[query_id]:\n",
    "                click = sessions_docs[doc_u]\n",
    "                summ += click + (1-click)*( (1-gammas[doc_u])*alphas[doc_u][query_num] ) / \\\n",
    "                                            (1-gammas[doc_u]*alphas[doc_u][query_num])\n",
    "            alphas[doc_u][query_num] = factor_1*summ\n",
    "    return alphas\n",
    "\n",
    "alphas = train_alphas(alphas, gammas, query_doc_pairs, n_docs)\n",
    "\n",
    "end = time.time()\n",
    "print(f'seconds spent {end - start}, mins spent {(end - start)/60}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is that true that we ve changed every alpha ?\n",
    "print('Is that true that we ve changed every alpha ? ->', not (alphas_old == alphas).any())\n",
    "assert alphas_old.shape == alphas.shape, 'new shapes are fucked up'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gammas\n",
    "\n",
    "We normalize by all the sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'query_doc_pairs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-93-9cc3f90151ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mall_sess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m \u001b[0mquery_doc_pairs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midd\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midd\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mquery_doc_pairs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mall_sess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_sess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# for comparison :\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mgammas_old\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgammas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'query_doc_pairs' is not defined"
     ]
    }
   ],
   "source": [
    "all_sess = [ query_doc_pairs[idd] for idd in query_doc_pairs]\n",
    "all_sess = reduce(lambda x,y: x+y, all_sess)\n",
    "\n",
    "# for comparison :\n",
    "gammas_old = gammas.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-94-c3affb6b7fd8>, line 22)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-94-c3affb6b7fd8>\"\u001b[0;36m, line \u001b[0;32m22\u001b[0m\n\u001b[0;31m    print(f'seconds spent {end - start}, mins spent {(end - start)/60}')\u001b[0m\n\u001b[0m                                                                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# Do the updates for gammas:\n",
    "\n",
    "def train_gammas(alphas, gammas, query_doc_pairs, n_docs):\n",
    "    for doc_u in range(n_docs):\n",
    "        # refer to alphas array with query_num as that it is how the are strored there:\n",
    "        summ =  0\n",
    "        for query_num, query_id in enumerate(query_doc_pairs):\n",
    "            factor_1 = 1 / len(all_sess)\n",
    "            for sessions_docs in query_doc_pairs[query_id]:\n",
    "                click = sessions_docs[doc_u]\n",
    "                summ += click + (1-click)*( (1-alphas[doc_u][query_num])*gammas[doc_u] ) / \\\n",
    "                                            (1-gammas[doc_u]*alphas[doc_u][query_num])        \n",
    "        gammas[doc_u] = factor_1 * summ\n",
    "    \n",
    "    return gammas\n",
    "\n",
    "gammas = train_gammas(alphas, gammas, query_doc_pairs, n_docs)\n",
    "\n",
    "end = time.time()\n",
    "print(f'seconds spent {end - start}, mins spent {(end - start)/60}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test to see if we have all queries filled in\n",
    "def check_sanity(l):\n",
    "    if l==[]:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "test = pd.Series(query_doc_pairs)\n",
    "test_res = test.apply(check_sanity)\n",
    "print('should be all true, coz otherwise we have queries with no sessions', np.unique(np.array(test_res)))\n",
    "\n",
    "assert gammas_old.shape == gammas.shape, 'shapes are fucked'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gammas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training multiple times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_gammas' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-95-57dff9714b07>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mgammas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_gammas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malphas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgammas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_doc_pairs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_docs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0malphas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_alphas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malphas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgammas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_doc_pairs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_docs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_gammas' is not defined"
     ]
    }
   ],
   "source": [
    "gammas = np.random.rand(n_docs)\n",
    "alphas = np.random.rand(n_docs*n_queries).reshape((n_docs,n_queries))\n",
    "\n",
    "#  Training till convergence: \n",
    "n_epochs = 100\n",
    "diff_alphas = [1]\n",
    "diff_gammas = [1]\n",
    "\n",
    "for i in range(n_epochs):\n",
    "    gammas = train_gammas(alphas, gammas, query_doc_pairs, n_docs)\n",
    "    alphas = train_alphas(alphas, gammas, query_doc_pairs, n_docs)\n",
    "    \n",
    "    #For plotting: checking convergence\n",
    "    diff_alphas.append( diff_alphas[-1] - sum(sum(alphas)) )\n",
    "    diff_gammas.append( diff_gammas[-1] - sum(gammas) )\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = np.linspace(0, n_epochs+1, n_epochs+1)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7,5))\n",
    "ax.plot(x,diff_gammas,marker='x')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.6491885 , 0.05822949, 0.08086501, 0.35289751, 0.39777152,\n",
       "       0.28309871])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gammas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5\n",
    "\n",
    "Power analysis:\n",
    "interleave and simulate clicks on every permutation for $k$ times. Then calculate winning prbability $p$. Do the Power Analysis to find minimum number of sample size $N$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables\n",
    "\n",
    "\n",
    "k = 50\n",
    "rnd.seed(21)\n",
    "gammas = [0.484221138516365, 0.21930976273093877, 0.1581402982275157]\n",
    "epsilon = 0.005\n",
    "\n",
    "p0 = 0.5\n",
    "alphaN = 0.05\n",
    "betaN = 0.1\n",
    "za = 0.8289\n",
    "zb = 0.8159\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if the probability of the PBM is higher than a ranomly generates number we assume a click happens\n",
    "\n",
    "def sim_clicks(rankList):\n",
    "    pList = np.multiply(np.ones(len(rankList)),gammas)\n",
    "    for n in range(len(pList)):\n",
    "        if rankList[n] == 1:\n",
    "            pList[n]*= (1-epsilon)\n",
    "        else:\n",
    "            pList[n]*= epsilon\n",
    "    randP = np.random.uniform(size=len(pList))\n",
    "    click = np.zeros(len(pList))\n",
    "    for n in range(len(pList)):\n",
    "        if pList[n] - randP[n] > 0:\n",
    "            click[n] = 1\n",
    "    return click\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_size(p1):\n",
    "    delta = np.absolute(p1-p0)\n",
    "    N1 = np.power(((za*np.square(p0*(1.0-p0))+zb*np.square(p1*(1.0-p1)))/delta),2)\n",
    "    N = np.ceil(N1 + (1/delta))\n",
    "    \n",
    "    return N\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', '0', '1', '1', '1', '1']\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (6,) (3,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-164-abaf5bb39415>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_possible_permutations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'111'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'101'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0mpoweranal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-164-abaf5bb39415>\u001b[0m in \u001b[0;36mpoweranal\u001b[0;34m(tup)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mrankP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprobabelistic_interleaving\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrel\u001b[0m \u001b[0;32min\u001b[0m  \u001b[0mrankP\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mclicksTD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msim_clicks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrel\u001b[0m \u001b[0;32min\u001b[0m  \u001b[0mrankTD\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mclicksP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msim_clicks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrel\u001b[0m \u001b[0;32min\u001b[0m  \u001b[0mrankP\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mclicksTDE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-119-d13add7e5af7>\u001b[0m in \u001b[0;36msim_clicks\u001b[0;34m(rankList)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msim_clicks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrankList\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mpList\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrankList\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgammas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpList\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrankList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (6,) (3,) "
     ]
    }
   ],
   "source": [
    "def poweranal(tup):\n",
    "    winsTDE = 0\n",
    "    winsPE = 0\n",
    "    winsTDP = 0\n",
    "    winsPP = 0\n",
    "    \n",
    "    for n in range(k):\n",
    "        rankTD = team_draft(tup)\n",
    "        rankTD = rankTD[:3]\n",
    "        rankP = probabelistic_interleaving(tup)\n",
    "        rankP = rankP[:3]\n",
    "        relTD = np.zeros(len(rankTD))\n",
    "        relP = np.zeros(len(rankP))\n",
    "        for m in range(len(rankTD)):\n",
    "            line = rankTD[m]\n",
    "            relTD[m] = line[1]\n",
    "        for m in range(len(rankP)):\n",
    "            line = rankP[m]\n",
    "            relP[m] = line[1]\n",
    "        clicksTD = sim_clicks(relTD)\n",
    "        clicksP = sim_clicks(relP)\n",
    "        clicksTDE = 0\n",
    "        clicksTDP = 0\n",
    "        clicksPE = 0\n",
    "        clicksPP = 0\n",
    "        for m in range(len(clicksTD)):\n",
    "            lineTD = rankTD[m]\n",
    "            lineP = rankP[m]\n",
    "            if clicksTD[m] == 1 and lineTD[2] == 'E':\n",
    "                clicksTDE +=1\n",
    "            if clicksTD[m] == 1 and lineTD[2] == 'P':\n",
    "                clicksTDP +=1\n",
    "            if clicksP[m] == 1 and lineP[2] == 'E':\n",
    "                clicksPE +=1\n",
    "            if clicksP[m] == 1 and lineP[2] == 'P':\n",
    "                clicksPP +=1\n",
    "        if clicksTDE>clicksTDP: winsTDE+=1\n",
    "        if clicksTDE<clicksTDP: winsTDP+=1 \n",
    "        if clicksPE>clicksPP:   winsPE+=1 \n",
    "        if clicksPE<clicksPP:   winsPE+=1 \n",
    "        \n",
    "    pTD = winsTDE/(winsTDE+winsTDP)\n",
    "    pP = winsPE/(winsPE+winsPP)\n",
    "    \n",
    "    # calcutale N\n",
    "    \n",
    "    NTD = sample_size(pTD)\n",
    "    NP = sample_size(pP)\n",
    "    \n",
    "    return NTD, NP\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', '0', '0', '0', '1', '0']\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (6,) (3,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-171-dde80356eb7f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mpermutations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_possible_permutations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrelevance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoclist\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpermutations\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m             \u001b[0mpoweranal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoclist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-164-abaf5bb39415>\u001b[0m in \u001b[0;36mpoweranal\u001b[0;34m(tup)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mrankP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprobabelistic_interleaving\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrel\u001b[0m \u001b[0;32min\u001b[0m  \u001b[0mrankP\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mclicksTD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msim_clicks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrel\u001b[0m \u001b[0;32min\u001b[0m  \u001b[0mrankTD\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mclicksP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msim_clicks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrel\u001b[0m \u001b[0;32min\u001b[0m  \u001b[0mrankP\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mclicksTDE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-119-d13add7e5af7>\u001b[0m in \u001b[0;36msim_clicks\u001b[0;34m(rankList)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msim_clicks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrankList\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mpList\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrankList\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgammas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpList\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrankList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (6,) (3,) "
     ]
    }
   ],
   "source": [
    "team_table = {}\n",
    "prob_table = {}\n",
    "for score,relevance_list in binned.items():\n",
    "    team_table[score] = []\n",
    "    prob_table[score] = []\n",
    "    for relevance in relevance_list:\n",
    "        permutations = create_possible_permutations(relevance)\n",
    "        for doclist in permutations:\n",
    "            team_sample,prob_sample = poweranal(doclist)\n",
    "            team_table[score].append(team_sample)\n",
    "            prob_table[score].append(prob_sample)\n",
    "            \n",
    "            \n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
