{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information Retrieval   |    Assignment 1-B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import random as rnd\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from functools import reduce\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Open file    \n",
    "fileHandler = open ('YandexRelPredChallenge.txt', \"r\")\n",
    "# Get list of all lines in file\n",
    "listOfLines = fileHandler.readlines()\n",
    "# Close file \n",
    "fileHandler.close()\n",
    "#Empty frame to fill in\n",
    "index=[x for x in range(0,100000)]\n",
    "df = pd.DataFrame(index=index, columns=['SessionID', 'TimePassed',\n",
    "                 'TypeOfAction', 'QueryID','RegionID', 'ListOfURLs', 'URLID'])\n",
    "df.name='data'\n",
    "\n",
    "# Iterate over the lines and fill in the dataframe\n",
    "for ind, line in zip(index, listOfLines):\n",
    "#     print('index')\n",
    "    line = line.strip().split()\n",
    "    if line[2]=='Q':\n",
    "\n",
    "        df.loc[ind] = pd.Series({'SessionID':line[0], 'TimePassed':line[1], 'TypeOfAction':line[2],\n",
    "                                     'QueryID':line[3],'RegionID':line[4], 'ListOfURLs':line[5:], 'URLID':np.nan})\n",
    "    else:\n",
    "        df.loc[ind] = pd.Series({'SessionID':line[0], 'TimePassed':line[1], 'TypeOfAction':line[2],\n",
    "                                     'QueryID':np.nan,'RegionID':np.nan, 'ListOfURLs':np.nan, 'URLID':line[3:]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the types right. \n",
    "df.SessionID = df.SessionID.astype(int)\n",
    "df.TimePassed = df.TimePassed.astype(int)\n",
    "df.TypeOfAction = df.TypeOfAction.astype(str)\n",
    "df.RegionID = df.RegionID.astype(float)\n",
    "df.TimePassed = df.TimePassed.astype(float)\n",
    "def parse_ulrid(x):\n",
    "    if type(x)==list:\n",
    "        return x[0]\n",
    "    else:\n",
    "        return x\n",
    "df.URLID = df.URLID.apply(parse_ulrid)\n",
    "df.URLID = df.URLID.astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create new relevance:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So lets say that our experimental system is what Yandex return as a list now. Production or the system that is going to 'suck' (sort of a dumb system that we are going to compare against) will be constructed as shuffling each ranking for a query. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, other approach, probably the better one: take the initial order as the 'benchmark' create only list of 0,1 (relevnace scores) for each system.\n",
    "    - rember that we are interested only in three [3] first results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new relevance scores \n",
    "relevances = [0,1]\n",
    "doc_length = 3 \n",
    "\n",
    "relevance_pairs=[\"\".join(seq) for seq in itertools.product(\"01\", repeat=6)]\n",
    "relevance_pairs = [(p[:3],p[3:])for p in relevance_pairs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Calculate ERR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ERR(l):\n",
    "    theta = [ (2**i - 1)/(2) for i in [0,1]]\n",
    "    err=0\n",
    "    \n",
    "    for rank in range(len(l)):\n",
    "        prod = 1\n",
    "        for idx in range(rank):\n",
    "            prod*=(1-theta[int(l[idx])])\n",
    "        prod*=(1/(rank+1))*theta[int(l[rank])]\n",
    "        err+=prod\n",
    "    return err\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "see the formuala for ERR at: http://olivier.chapelle.cc/pub/err.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the deltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "binned = { x/100:[]for x in range(10,100,5) }#all below 0.05 is discarded The numbers are the upperbound\n",
    "for tup in relevance_pairs:\n",
    "    d_err = ERR(tup[0]) - ERR(tup[1])\n",
    "    if d_err<0.05:\n",
    "        continue #discard this entry\n",
    "    \n",
    "    d_err =int(5 * round((d_err*100)/5))/100\n",
    "    binned[d_err].append(tup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_possible_permutations(tup):\n",
    "    ids=['a','b','c','d','e','f']\n",
    "    pair1 = ['a','b','c']\n",
    "    rel1  = tup[0]\n",
    "    rel2  = tup[1]\n",
    "    possible_pairs = []\n",
    "    \n",
    "    possible_ids = []\n",
    "    for idx in range(3):\n",
    "        possible_ids.append([ids[idx+3]])\n",
    "        for _id,r1 in enumerate(rel1):\n",
    "                if r1==rel2[idx]:\n",
    "                    possible_ids[idx].append(ids[_id])\n",
    "                    \n",
    "    for l0 in possible_ids[0]:\n",
    "        for l1 in possible_ids[1]:\n",
    "            for l2 in possible_ids[2]:\n",
    "                if l0 is not l1 is not l2 is not l0:\n",
    "                    possible_pairs.append([l0,l1,l2])\n",
    "    pairs = []\n",
    "    for p in possible_pairs:\n",
    "        pairs.append((pair1,p))\n",
    "    return pairs\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def team_draft(tup,start=0,):\n",
    "    l = []\n",
    "    tup = (list(tup[0]),list(tup[1]))\n",
    "    for idx in range(6):\n",
    "        start+=1\n",
    "        if len(tup[start%2])==0:continue\n",
    "        elem = tup[start%2].pop(0)\n",
    "        tup = (list(filter(lambda a: a != elem, tup[0])), list(filter(lambda a: a != elem, tup[1])))\n",
    "        l.append((elem,start%2))\n",
    "    return l\n",
    "\n",
    "def probabelistic_interleaving(tup):\n",
    "    l = []\n",
    "    tup = (list(tup[0]),list(tup[1]))\n",
    "\n",
    "    for x in range(6):\n",
    "        active=[]\n",
    "        if(len(tup[0]))>0:active+=[0]\n",
    "        if(len(tup[1]))>0:active+=[1]    \n",
    "        if not active:break\n",
    "        cur = np.random.choice(active)#get the currently selected model\n",
    "        \n",
    "        unnormalized_probs=[]\n",
    "        for idx in range(len(tup[cur])):\n",
    "            unnormalized_probs.append(1/((idx+1)**3))#tau is 3 from the paper\n",
    "        unnormalized_probs = [x/sum(unnormalized_probs) for x in unnormalized_probs]\n",
    "        elem = np.random.choice(tup[cur],p=unnormalized_probs)\n",
    "        tup = (list(filter(lambda a: a != elem, tup[0])), list(filter(lambda a: a != elem, tup[1])))\n",
    "        l.append((elem,cur))\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = create_possible_permutations(['111','111'])\n",
    "print(probabelistic_interleaving(l[10]))\n",
    "print(team_draft(l[10]))\n",
    "\n",
    "print(l[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4.1: Estimating the params of click models: Random Click Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13445559411047547\n"
     ]
    }
   ],
   "source": [
    "# Get all the doc ids\n",
    "data = df.drop(columns=['id_rel_e', 'id_rel_p', 'rel_e', 'rel_p', 'id_e',\n",
    "       'id_p', 'err_e', 'err_p'])\n",
    "data.QueryID = data.QueryID.astype(float)\n",
    "list_doc_ids_raw = data.ListOfURLs[df.ListOfURLs.isnull()!=True].values\n",
    "res = list()\n",
    "for i in list_doc_ids_raw:\n",
    "    res += i\n",
    "all_doc_ids = res.copy()\n",
    "# Get all the clicks\n",
    "list_clicks_ids = data.URLID[df.URLID.isnull()!=True].values\n",
    "\n",
    "# rho = probability of a doc to be clicked.\n",
    "rho = len(list_clicks_id)/len(all_doc_ids)\n",
    "print(rho)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4.2: Estimating the params of click models: Posistion-based Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the data for training\n",
    "    - assumption: we assume that all the queries have at least 3 docs shown.\n",
    "    \n",
    "    TODO: check if true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.copy()\n",
    "data.QueryID = data.QueryID.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the tables with all the params\n",
    "n_docs = 3\n",
    "unique_queries = data.QueryID[data.QueryID.isnull()!=True].astype(int).unique()\n",
    "n_queries = len(unique_queries) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "suka bleyat, but still kinda quick tho : seconds spent 89.54805207252502, mins spent 1.4924675345420837\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# Create query_doc empty pairs\n",
    "query_doc_pairs = defaultdict()\n",
    "for i in unique_queries:\n",
    "    query_doc_pairs[i]=[]\n",
    "    \n",
    "# This code will give an error at the very last session ! -> move last session to the different bit.\n",
    "sessions = data.SessionID.unique()[:-1]\n",
    "for session in sessions:\n",
    "    data_session = data[data.SessionID==session]\n",
    "    click_ids =  data_session.URLID[data_session.URLID.isnull()!=True].values\n",
    "    query_ids = data_session.QueryID[data_session.QueryID.isnull()!=True].values\n",
    "    for query_id in query_ids:\n",
    "        first_3_docs = data_session.ListOfURLs[data.QueryID==query_id].iloc[0]\n",
    "        first_3_docs = np.array(list(map(int,first_3_docs))[:3])\n",
    "        # take index in the current data_session\n",
    "        query_id_ind = data_session.QueryID[data_session.QueryID==query_id].index[0]\n",
    "        flag = not math.isnan(data.URLID.iloc[query_id_ind+1])\n",
    "        # if the next entry is click\n",
    "        if flag:\n",
    "            click_log = np.array([0,0,0])\n",
    "            query_id_ind+=1\n",
    "            while flag==1:\n",
    "                click_urlid = data.URLID.iloc[query_id_ind]\n",
    "                # if click_urlid in first_3_docs then click is 1 on that position.\n",
    "                doc_u_id = np.where(first_3_docs==click_urlid)\n",
    "                click_log[doc_u_id]=1\n",
    "                # increase the count\n",
    "                query_id_ind +=1\n",
    "                flag = not math.isnan(data.URLID.iloc[query_id_ind])\n",
    "            # do updates\n",
    "        else:\n",
    "            click_log = np.array([0,0,0])\n",
    "            # do updates\n",
    "            \n",
    "        # Save query_id-click_log (=session-doc) pair for each query_id\n",
    "        query_doc_pairs[query_id].append(click_log)\n",
    "    \n",
    "end = time.time()\n",
    "print(f'suka bleyat, but still kinda quick tho : seconds spent {end - start}, mins spent {(end - start)/60}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "query_doc_pairs_old = query_doc_pairs.copy()\n",
    "# query_doc_pairs_old"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform EM steps (update assignments ; update params):\n",
    "    \n",
    "    1) initialize alpha and gamma at random \n",
    "    2) For each time step:\n",
    "        - Expecation: do not really have to do this step [?], what we want to see at the end: P(C|u) \n",
    "            - calculate P(C|u) for each u (u is from 1 to 3 = positions in the shown list)\n",
    "            - \" P(C|u) = P(A_uq|E,u)P(E|u) \"\n",
    "                \n",
    "        - Maximization: update alpha and gamma params\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alphas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = np.random.rand(n_docs*n_queries).reshape((n_docs,n_queries))\n",
    "\n",
    "assert unique_queries.shape[0] == alphas.shape[1], 'Error, bleayt'\n",
    "\n",
    "# for comparison :\n",
    "alphas_old = alphas.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seconds spent 1.0285494327545166, mins spent 0.01714249054590861\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# Do the updates for alphas:\n",
    "\n",
    "def train_alphas(alphas, gammas, query_doc_pairs, n_docs):\n",
    "    # refer to alphas array with query_num as that it is how the are strored there:\n",
    "    for query_num, query_id in enumerate(query_doc_pairs):\n",
    "        for doc_u in range(n_docs):\n",
    "            factor_1 = 1/len(query_doc_pairs[query_id])\n",
    "            summ =  0\n",
    "            for sessions_docs in query_doc_pairs[query_id]:\n",
    "                click = sessions_docs[doc_u]\n",
    "                summ += click + (1-click)*( (1-gammas[doc_u])*alphas[doc_u][query_num] ) / \\\n",
    "                                            (1-gammas[doc_u]*alphas[doc_u][query_num])\n",
    "            alphas[doc_u][query_num] = factor_1*summ\n",
    "    return alphas\n",
    "\n",
    "alphas = train_alphas(alphas, gammas, query_doc_pairs, n_docs)\n",
    "\n",
    "end = time.time()\n",
    "print(f'seconds spent {end - start}, mins spent {(end - start)/60}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is that true that we ve changed every alpha ? -> True\n"
     ]
    }
   ],
   "source": [
    "# Is that true that we ve changed every alpha ?\n",
    "print('Is that true that we ve changed every alpha ? ->', not (alphas_old == alphas).any())\n",
    "assert alphas_old.shape == alphas.shape, 'new shapes are fucked up'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gammas\n",
    "\n",
    "We normalize by all the sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "gammas = np.random.rand(n_docs)\n",
    "\n",
    "all_sess = [ query_doc_pairs[idd] for idd in query_doc_pairs]\n",
    "all_sess = reduce(lambda x,y: x+y, all_sess)\n",
    "\n",
    "# for comparison :\n",
    "gammas_old = gammas.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seconds spent 1.023488998413086, mins spent 0.017058149973551432\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# Do the updates for gammas:\n",
    "\n",
    "def train_gammas(alphas, gammas, query_doc_pairs, n_docs):\n",
    "    for doc_u in range(n_docs):\n",
    "        # refer to alphas array with query_num as that it is how the are strored there:\n",
    "        summ =  0\n",
    "        for query_num, query_id in enumerate(query_doc_pairs):\n",
    "            factor_1 = 1 / len(all_sess)\n",
    "            for sessions_docs in query_doc_pairs[query_id]:\n",
    "                click = sessions_docs[doc_u]\n",
    "                summ += click + (1-click)*( (1-alphas[doc_u][query_num])*gammas[doc_u] ) / \\\n",
    "                                            (1-gammas[doc_u]*alphas[doc_u][query_num])        \n",
    "        gammas[doc_u] = factor_1 * summ\n",
    "    \n",
    "    return gammas\n",
    "\n",
    "gammas = train_gammas(alphas, gammas, query_doc_pairs, n_docs)\n",
    "\n",
    "end = time.time()\n",
    "print(f'seconds spent {end - start}, mins spent {(end - start)/60}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "should be all true, coz otherwise we have queries with no sessions [ True]\n"
     ]
    }
   ],
   "source": [
    "# Test to see if we have all queries filled in\n",
    "def check_sanity(l):\n",
    "    if l==[]:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "test = pd.Series(query_doc_pairs)\n",
    "test_res = test.apply(check_sanity)\n",
    "print('should be all true, coz otherwise we have queries with no sessions', np.unique(np.array(test_res)))\n",
    "\n",
    "assert gammas_old.shape == gammas.shape, 'shapes are fucked'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training multiple times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marr/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:13: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "gammas = np.random.rand(n_docs)\n",
    "alphas = np.random.rand(n_docs*n_queries).reshape((n_docs,n_queries))\n",
    "\n",
    "#  Training till convergence: \n",
    "n_epochs = 100\n",
    "diff_alphas = [1]\n",
    "diff_gammas = [1]\n",
    "\n",
    "for i in range(n_epochs):\n",
    "    gammas = train_gammas(alphas, gammas, query_doc_pairs, n_docs)\n",
    "    alphas = train_alphas(alphas, gammas, query_doc_pairs, n_docs)\n",
    "    \n",
    "    #For plotting: checking convergence\n",
    "    diff_alphas.append( diff_alphas[-1] - sum(sum(alphas)) )\n",
    "    diff_gammas.append( diff_gammas[-1] - sum(gammas) )\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = np.linspace(0, n_epochs+1, n_epochs+1)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7,5))\n",
    "ax.plot(x,diff_gammas,marker='x')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_alphas = list(map(sum,diff_alphas[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.        , 0.9999976 , 0.99994865])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gammas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
